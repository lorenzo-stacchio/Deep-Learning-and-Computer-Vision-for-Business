{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yd2Z_BNCCWzf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSrJ1Lx_CWzh"
      },
      "source": [
        "# Fine-Tuning and Feature Extraction with PyTorch Models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "204O3G7dCWzi"
      },
      "source": [
        "In this tutorial, we will take a deeper look at how to fine-tune and perform feature extraction using `torchvision models <https://pytorch.org/docs/stable/torchvision/models.html>`, all of which have been pretrained on the 1000-class ImageNet dataset. This tutorial will provide an in-depth understanding of how to work with several modern Convolutional Neural Network (CNN) architectures and develop intuition for fine-tuning any PyTorch model. Since each model architecture differs, there is no universal fine-tuning code that applies to all cases. Instead, researchers must inspect the architecture and make custom adjustments for each model.\n",
        "\n",
        "In this document, we will explore two types of transfer learning: **fine-tuning** and **feature extraction**.\n",
        "\n",
        "- **Fine-tuning**: We start with a pretrained model and update *all* of the modelâ€™s parameters for the new task, essentially retraining the entire model.\n",
        "- **Feature extraction**: We begin with a pretrained model and only update the final layer's weights to make predictions for the new task. It is called feature extraction because we use the pretrained CNN as a fixed feature extractor, modifying only the output layer.\n",
        "\n",
        "For more technical details about transfer learning, you can refer to these resources: [Transfer Learning in CS231n](http://cs231n.github.io/transfer-learning/).\n",
        "\n",
        "#### General Steps for Transfer Learning\n",
        "\n",
        "Both transfer learning methods follow these common steps:\n",
        "\n",
        "1. Initialize the pretrained model.\n",
        "2. Reshape the final layer(s) to match the number of output classes in the new dataset.\n",
        "3. Define which parameters will be updated during the optimization process.\n",
        "4. Run the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g54vrQimCWzi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.12.1+cu113\n",
            "Torchvision Version:  0.13.1+cu113\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guVw9zafCWzj"
      },
      "source": [
        "Inputs\n",
        "------\n",
        "\n",
        "Here are all of the parameters to change for the run. We will use the\n",
        "*hymenoptera_data* dataset which can be downloaded\n",
        "`here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`__.\n",
        "This dataset contains two classes, **bees** and **ants**, and is\n",
        "structured such that we can use the\n",
        "`ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`__\n",
        "dataset, rather than writing our own custom dataset. Download the data\n",
        "and set the ``data_dir`` input to the root directory of the dataset. The\n",
        "``model_name`` input is the name of the model you wish to use and must\n",
        "be selected from this list:\n",
        "\n",
        "::\n",
        "\n",
        "   [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "\n",
        "The other inputs are as follows: ``num_classes`` is the number of\n",
        "classes in the dataset, ``batch_size`` is the batch size used for\n",
        "training and may be adjusted according to the capability of your\n",
        "machine, ``num_epochs`` is the number of training epochs we want to run,\n",
        "and ``feature_extract`` is a boolean that defines if we are finetuning\n",
        "or feature extracting. If ``feature_extract = False``, the model is\n",
        "finetuned and all model parameters are updated. If\n",
        "``feature_extract = True``, only the last layer parameters are updated,\n",
        "the others remain fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v6zlyHEtCWzj"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "\n",
        "model_name = \"resnet50\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 15\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import pandas as pd\n",
        "from PIL import Image \n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Custom dataset class to handle the XML annotations\n",
        "class RetailProductDataset(Dataset):\n",
        "    \n",
        "    def _convert_image_to_rgb(self, image):\n",
        "        return image.convert(\"RGB\")\n",
        "    \n",
        "    def get_transform(self):\n",
        "        if self.partition == \"train\":\n",
        "            return transforms.Compose([\n",
        "                self._convert_image_to_rgb,\n",
        "                transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "                transforms.RandomCrop(224),\n",
        "                # Randomly flip the image horizontally\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                # Randomly flip the image horizontally\n",
        "                # transforms.ColorJitter(\n",
        "                #     brightness=0.4, contrast=0.2, saturation=0.2),\n",
        "                # Randomly flip the image horizontallys\n",
        "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.2),\n",
        "                transforms.ToTensor(),\n",
        "                # # Normalize with mean and std of imagenet\n",
        "                transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                                    (0.26862954, 0.26130258, 0.27577711)),\n",
        "            ])\n",
        "        else:\n",
        "            return transforms.Compose([\n",
        "                self._convert_image_to_rgb,\n",
        "                transforms.Resize((224,224), interpolation=InterpolationMode.BICUBIC),\n",
        "                # transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                # Normalize with mean and std of imagenet\n",
        "                transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                                    (0.26862954, 0.26130258, 0.27577711)),\n",
        "            ])\n",
        "\n",
        "    def denormalize(self, image):\n",
        "        mean, std = (0.48145466, 0.4578275,\n",
        "                     0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "        mean = torch.tensor(mean).view(1, 3, 1, 1)\n",
        "        std = torch.tensor(std).view(1, 3, 1, 1)\n",
        "\n",
        "        return image * std + mean\n",
        "    \n",
        "    def __init__(self, root_folder, partition, bbox = False):\n",
        "        self.data = []\n",
        "        self.root_folder = root_folder\n",
        "        self.partition = partition\n",
        "        self.bbox = bbox\n",
        "        self.annotation_folder  = f\"{self.root_folder}/annotations/{self.partition}/\"\n",
        "        self.image_folder  = f\"{self.root_folder}/images/{self.partition}/\"\n",
        "        self.dataframe = self.generate_dataframe()\n",
        "        self.list_cls = list(self.dataframe[\"cls\"].unique())\n",
        "        self.cls_dict = {name:idx for idx, name in enumerate(sorted(self.list_cls))}\n",
        "        \n",
        "    def generate_dataframe(self):\n",
        "        # print(self.image_folder + \"*.jpg\")\n",
        "        image_paths = [x for x in glob.glob(self.image_folder + \"*.jpg\")]\n",
        "        annotation_paths = [x for x in glob.glob(self.annotation_folder + \"*.xml\")]\n",
        "        image_paths = sorted(image_paths, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
        "        annotation_paths = sorted(annotation_paths, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
        "        assert np.all(i == a for i, a in zip(image_paths,annotation_paths))\n",
        "        # print(image_paths)\n",
        "        # print(annotation_paths)\n",
        "\n",
        "        df = pd.DataFrame(columns = [\"image_path\", \"width\", \"height\", \"cls\", \"bbox\"])\n",
        "        for i,a in zip(image_paths,annotation_paths):\n",
        "            annotations = self.parse_xml(a)\n",
        "            # print(i,a)\n",
        "            df = df._append({\"image_path\": i, \"width\":annotations[\"width\"], \n",
        "                            \"height\":annotations[\"height\"], \"cls\":annotations[\"cls\"], \"bbox\":annotations[\"bbox\"]},ignore_index = True)\n",
        "        return df\n",
        "        \n",
        "    def parse_xml(self, xml_file):\n",
        "        # Parse XML content\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract the relevant fields from the XML\n",
        "        # filename = root.find('filename').text\n",
        "        # path = root.find('path').text\n",
        "        size = root.find('size')\n",
        "        width = int(size.find('width').text)\n",
        "        height = int(size.find('height').text)\n",
        "        channels = int(size.find('depth').text)\n",
        "        \n",
        "        ## Object Detection info\n",
        "        obj = root.find('object')\n",
        "        cls = obj.find('name').text\n",
        "        bndbox = obj.find('bndbox')\n",
        "        xmin = int(bndbox.find('xmin').text)\n",
        "        ymin = int(bndbox.find('ymin').text)\n",
        "        xmax = int(bndbox.find('xmax').text)\n",
        "        ymax = int(bndbox.find('ymax').text)\n",
        "\n",
        "        # Store the parsed data\n",
        "        return {\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'channels': channels,\n",
        "            'cls': cls,\n",
        "            'bbox': [xmin, ymin, xmax, ymax]\n",
        "        }\n",
        "\n",
        "    def __datasetclasses__(self):\n",
        "        return len(self.list_cls)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataframe.loc[idx]\n",
        "        image_path = item[\"image_path\"]  # read path\n",
        "        bbox_items = item[\"bbox\"]\n",
        "        image = Image.open(image_path)\n",
        "       \n",
        "        if self.bbox:\n",
        "            ## crop bbox\n",
        "            print(\"-----------------------------\")\n",
        "            print(image_path)\n",
        "            print(os.path.basename(image_path))\n",
        "            print(image.size)\n",
        "            print((bbox_items[0],bbox_items[1],bbox_items[2],bbox_items[3]))\n",
        "            print(\"-----------------------------\")\n",
        "            image  = image.crop((bbox_items[0],bbox_items[1],bbox_items[2],bbox_items[3]))\n",
        "\n",
        "        # mask_image = Image.open(mask_name)\n",
        "        try:\n",
        "            transform = self.get_transform()\n",
        "            image = transform(image)\n",
        "        except Exception as e:\n",
        "            # print(e)\n",
        "            print(\"\\n\\n NAME\", image_path)\n",
        "        cls = self.cls_dict[item[\"cls\"]]\n",
        "        return {\"image_path\" : image_path, \"image\": image, \"cls\": cls}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "294\n",
            "86\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (1).jpg\n",
            "aqua (1).jpg\n",
            "(512, 512)\n",
            "(191, 142, 257, 332)\n",
            "-----------------------------\n",
            "aqua (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (1).jpg\n",
            "chitato (1).jpg\n",
            "(512, 512)\n",
            "(86, 171, 345, 376)\n",
            "-----------------------------\n",
            "chitato (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (1).jpg\n",
            "indomie (1).jpg\n",
            "(512, 512)\n",
            "(91, 187, 312, 354)\n",
            "-----------------------------\n",
            "indomie (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (1).jpg\n",
            "mix (1).jpg\n",
            "(512, 512)\n",
            "(64, 193, 127, 274)\n",
            "-----------------------------\n",
            "mix (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (1).jpg\n",
            "pepsodent (1).jpg\n",
            "(512, 512)\n",
            "(60, 200, 444, 300)\n",
            "-----------------------------\n",
            "pepsodent (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (1).jpg\n",
            "shampoo (1).jpg\n",
            "(512, 512)\n",
            "(110, 180, 310, 320)\n",
            "-----------------------------\n",
            "shampoo (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (1).jpg\n",
            "tissue (1).jpg\n",
            "(512, 512)\n",
            "(62, 173, 463, 373)\n",
            "-----------------------------\n",
            "tissue (1).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (2).jpg\n",
            "aqua (2).jpg\n",
            "(512, 512)\n",
            "(108, 60, 247, 326)\n",
            "-----------------------------\n",
            "aqua (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (2).jpg\n",
            "chitato (2).jpg\n",
            "(512, 512)\n",
            "(73, 135, 446, 394)\n",
            "-----------------------------\n",
            "chitato (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (2).jpg\n",
            "indomie (2).jpg\n",
            "(512, 512)\n",
            "(12, 35, 267, 209)\n",
            "-----------------------------\n",
            "indomie (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (2).jpg\n",
            "mix (2).jpg\n",
            "(512, 512)\n",
            "(84, 124, 228, 273)\n",
            "-----------------------------\n",
            "mix (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (2).jpg\n",
            "pepsodent (2).jpg\n",
            "(512, 512)\n",
            "(180, 0, 300, 480)\n",
            "-----------------------------\n",
            "pepsodent (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (2).jpg\n",
            "shampoo (2).jpg\n",
            "(512, 512)\n",
            "(350, 100, 512, 420)\n",
            "-----------------------------\n",
            "shampoo (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (2).jpg\n",
            "tissue (2).jpg\n",
            "(512, 512)\n",
            "(74, 154, 444, 353)\n",
            "-----------------------------\n",
            "tissue (2).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (3).jpg\n",
            "aqua (3).jpg\n",
            "(512, 512)\n",
            "(171, 122, 287, 401)\n",
            "-----------------------------\n",
            "aqua (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (3).jpg\n",
            "chitato (3).jpg\n",
            "(512, 512)\n",
            "(7, 122, 263, 393)\n",
            "-----------------------------\n",
            "chitato (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (3).jpg\n",
            "indomie (3).jpg\n",
            "(512, 512)\n",
            "(33, 60, 197, 235)\n",
            "-----------------------------\n",
            "indomie (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (3).jpg\n",
            "mix (3).jpg\n",
            "(512, 512)\n",
            "(76, 150, 218, 246)\n",
            "-----------------------------\n",
            "mix (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (3).jpg\n",
            "pepsodent (3).jpg\n",
            "(512, 512)\n",
            "(192, 11, 303, 510)\n",
            "-----------------------------\n",
            "pepsodent (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (3).jpg\n",
            "shampoo (3).jpg\n",
            "(512, 512)\n",
            "(150, 30, 450, 200)\n",
            "-----------------------------\n",
            "shampoo (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (3).jpg\n",
            "tissue (3).jpg\n",
            "(512, 512)\n",
            "(155, 80, 303, 407)\n",
            "-----------------------------\n",
            "tissue (3).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (4).jpg\n",
            "aqua (4).jpg\n",
            "(512, 512)\n",
            "(205, 81, 317, 360)\n",
            "-----------------------------\n",
            "aqua (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (4).jpg\n",
            "chitato (4).jpg\n",
            "(512, 512)\n",
            "(95, 78, 398, 460)\n",
            "-----------------------------\n",
            "chitato (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (4).jpg\n",
            "indomie (4).jpg\n",
            "(512, 512)\n",
            "(152, 167, 381, 357)\n",
            "-----------------------------\n",
            "indomie (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (4).jpg\n",
            "mix (4).jpg\n",
            "(512, 512)\n",
            "(120, 130, 185, 259)\n",
            "-----------------------------\n",
            "mix (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (4).jpg\n",
            "pepsodent (4).jpg\n",
            "(512, 512)\n",
            "(140, 50, 400, 370)\n",
            "-----------------------------\n",
            "pepsodent (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (4).jpg\n",
            "shampoo (4).jpg\n",
            "(512, 512)\n",
            "(170, 61, 365, 500)\n",
            "-----------------------------\n",
            "shampoo (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (4).jpg\n",
            "tissue (4).jpg\n",
            "(512, 512)\n",
            "(167, 119, 311, 450)\n",
            "-----------------------------\n",
            "tissue (4).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (5).jpg\n",
            "aqua (5).jpg\n",
            "(512, 512)\n",
            "(247, 196, 360, 497)\n",
            "-----------------------------\n",
            "aqua (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (5).jpg\n",
            "chitato (5).jpg\n",
            "(512, 512)\n",
            "(101, 210, 445, 379)\n",
            "-----------------------------\n",
            "chitato (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (5).jpg\n",
            "indomie (5).jpg\n",
            "(512, 512)\n",
            "(177, 180, 392, 398)\n",
            "-----------------------------\n",
            "indomie (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (5).jpg\n",
            "mix (5).jpg\n",
            "(512, 512)\n",
            "(1, 140, 78, 305)\n",
            "-----------------------------\n",
            "mix (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (5).jpg\n",
            "pepsodent (5).jpg\n",
            "(512, 512)\n",
            "(100, 180, 363, 270)\n",
            "-----------------------------\n",
            "pepsodent (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (5).jpg\n",
            "shampoo (5).jpg\n",
            "(512, 512)\n",
            "(170, 63, 327, 475)\n",
            "-----------------------------\n",
            "shampoo (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (5).jpg\n",
            "tissue (5).jpg\n",
            "(512, 512)\n",
            "(83, 154, 374, 304)\n",
            "-----------------------------\n",
            "tissue (5).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (6).jpg\n",
            "aqua (6).jpg\n",
            "(512, 512)\n",
            "(127, 32, 283, 447)\n",
            "-----------------------------\n",
            "aqua (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (6).jpg\n",
            "chitato (6).jpg\n",
            "(512, 512)\n",
            "(119, 161, 325, 421)\n",
            "-----------------------------\n",
            "chitato (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (6).jpg\n",
            "indomie (6).jpg\n",
            "(512, 512)\n",
            "(157, 73, 414, 333)\n",
            "-----------------------------\n",
            "indomie (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (6).jpg\n",
            "mix (6).jpg\n",
            "(512, 512)\n",
            "(39, 185, 151, 288)\n",
            "-----------------------------\n",
            "mix (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (6).jpg\n",
            "pepsodent (6).jpg\n",
            "(512, 512)\n",
            "(96, 199, 325, 273)\n",
            "-----------------------------\n",
            "pepsodent (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (6).jpg\n",
            "shampoo (6).jpg\n",
            "(512, 512)\n",
            "(150, 100, 324, 350)\n",
            "-----------------------------\n",
            "shampoo (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (6).jpg\n",
            "tissue (6).jpg\n",
            "(512, 512)\n",
            "(11, 137, 323, 454)\n",
            "-----------------------------\n",
            "tissue (6).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (7).jpg\n",
            "aqua (7).jpg\n",
            "(512, 512)\n",
            "(180, 62, 306, 429)\n",
            "-----------------------------\n",
            "aqua (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (7).jpg\n",
            "chitato (7).jpg\n",
            "(512, 512)\n",
            "(109, 86, 381, 413)\n",
            "-----------------------------\n",
            "chitato (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (7).jpg\n",
            "indomie (7).jpg\n",
            "(512, 512)\n",
            "(65, 298, 414, 403)\n",
            "-----------------------------\n",
            "indomie (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (7).jpg\n",
            "mix (7).jpg\n",
            "(512, 512)\n",
            "(148, 125, 320, 237)\n",
            "-----------------------------\n",
            "mix (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (7).jpg\n",
            "pepsodent (7).jpg\n",
            "(512, 512)\n",
            "(133, 126, 328, 430)\n",
            "-----------------------------\n",
            "pepsodent (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (7).jpg\n",
            "shampoo (7).jpg\n",
            "(512, 512)\n",
            "(171, 128, 258, 284)\n",
            "-----------------------------\n",
            "shampoo (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (7).jpg\n",
            "tissue (7).jpg\n",
            "(512, 512)\n",
            "(192, 106, 375, 357)\n",
            "-----------------------------\n",
            "tissue (7).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (8).jpg\n",
            "aqua (8).jpg\n",
            "(512, 512)\n",
            "(211, 89, 345, 507)\n",
            "-----------------------------\n",
            "aqua (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (8).jpg\n",
            "chitato (8).jpg\n",
            "(512, 512)\n",
            "(60, 185, 497, 335)\n",
            "-----------------------------\n",
            "chitato (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (8).jpg\n",
            "indomie (8).jpg\n",
            "(512, 512)\n",
            "(66, 288, 447, 417)\n",
            "-----------------------------\n",
            "indomie (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (8).jpg\n",
            "mix (8).jpg\n",
            "(512, 512)\n",
            "(72, 136, 147, 257)\n",
            "-----------------------------\n",
            "mix (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (8).jpg\n",
            "pepsodent (8).jpg\n",
            "(512, 512)\n",
            "(110, 147, 370, 421)\n",
            "-----------------------------\n",
            "pepsodent (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (8).jpg\n",
            "shampoo (8).jpg\n",
            "(512, 512)\n",
            "(184, 100, 316, 476)\n",
            "-----------------------------\n",
            "shampoo (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (8).jpg\n",
            "tissue (8).jpg\n",
            "(512, 512)\n",
            "(76, 181, 436, 393)\n",
            "-----------------------------\n",
            "tissue (8).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (9).jpg\n",
            "aqua (9).jpg\n",
            "(512, 512)\n",
            "(181, 114, 442, 493)\n",
            "-----------------------------\n",
            "aqua (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (9).jpg\n",
            "chitato (9).jpg\n",
            "(512, 512)\n",
            "(190, 110, 338, 454)\n",
            "-----------------------------\n",
            "chitato (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (9).jpg\n",
            "indomie (9).jpg\n",
            "(512, 512)\n",
            "(213, 73, 310, 400)\n",
            "-----------------------------\n",
            "indomie (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (9).jpg\n",
            "mix (9).jpg\n",
            "(512, 512)\n",
            "(71, 141, 160, 268)\n",
            "-----------------------------\n",
            "mix (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (9).jpg\n",
            "pepsodent (9).jpg\n",
            "(512, 512)\n",
            "(73, 195, 445, 292)\n",
            "-----------------------------\n",
            "pepsodent (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (9).jpg\n",
            "shampoo (9).jpg\n",
            "(512, 512)\n",
            "(219, 92, 313, 508)\n",
            "-----------------------------\n",
            "shampoo (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\tissue (9).jpg\n",
            "tissue (9).jpg\n",
            "(512, 512)\n",
            "(146, 178, 380, 445)\n",
            "-----------------------------\n",
            "tissue (9).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\aqua (10).jpg\n",
            "aqua (10).jpg\n",
            "(512, 512)\n",
            "(142, 82, 344, 495)\n",
            "-----------------------------\n",
            "aqua (10).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\chitato (10).jpg\n",
            "chitato (10).jpg\n",
            "(512, 512)\n",
            "(111, 195, 413, 316)\n",
            "-----------------------------\n",
            "chitato (10).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (10).jpg\n",
            "indomie (10).jpg\n",
            "(512, 512)\n",
            "(190, 120, 306, 479)\n",
            "-----------------------------\n",
            "indomie (10).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (10).jpg\n",
            "mix (10).jpg\n",
            "(512, 512)\n",
            "(1, 157, 149, 400)\n",
            "-----------------------------\n",
            "mix (10).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (10).jpg\n",
            "pepsodent (10).jpg\n",
            "(512, 512)\n",
            "(92, 250, 433, 350)\n",
            "-----------------------------\n",
            "pepsodent (10).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\shampoo (10).jpg\n",
            "shampoo (10).jpg\n",
            "(512, 512)\n",
            "(201, 267, 316, 343)\n",
            "-----------------------------\n",
            "shampoo (10).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\indomie (11).jpg\n",
            "indomie (11).jpg\n",
            "(512, 512)\n",
            "(96, 176, 423, 429)\n",
            "-----------------------------\n",
            "indomie (11).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (11).jpg\n",
            "mix (11).jpg\n",
            "(512, 512)\n",
            "(125, 186, 238, 270)\n",
            "-----------------------------\n",
            "mix (11).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (12).jpg\n",
            "mix (12).jpg\n",
            "(512, 512)\n",
            "(174, 123, 259, 217)\n",
            "-----------------------------\n",
            "mix (12).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (13).jpg\n",
            "mix (13).jpg\n",
            "(512, 512)\n",
            "(133, 93, 350, 146)\n",
            "-----------------------------\n",
            "mix (13).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (14).jpg\n",
            "mix (14).jpg\n",
            "(512, 512)\n",
            "(79, 91, 178, 320)\n",
            "-----------------------------\n",
            "mix (14).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (15).jpg\n",
            "mix (15).jpg\n",
            "(512, 512)\n",
            "(61, 93, 227, 267)\n",
            "-----------------------------\n",
            "mix (15).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (16).jpg\n",
            "mix (16).jpg\n",
            "(512, 512)\n",
            "(140, 130, 223, 218)\n",
            "-----------------------------\n",
            "mix (16).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (17).jpg\n",
            "mix (17).jpg\n",
            "(512, 512)\n",
            "(17, 70, 223, 261)\n",
            "-----------------------------\n",
            "mix (17).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (18).jpg\n",
            "mix (18).jpg\n",
            "(512, 512)\n",
            "(71, 253, 212, 405)\n",
            "-----------------------------\n",
            "mix (18).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (19).jpg\n",
            "mix (19).jpg\n",
            "(512, 512)\n",
            "(147, 31, 292, 217)\n",
            "-----------------------------\n",
            "mix (19).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (20).jpg\n",
            "mix (20).jpg\n",
            "(512, 512)\n",
            "(138, 185, 305, 303)\n",
            "-----------------------------\n",
            "mix (20).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (21).jpg\n",
            "mix (21).jpg\n",
            "(512, 512)\n",
            "(47, 191, 220, 403)\n",
            "-----------------------------\n",
            "mix (21).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (22).jpg\n",
            "mix (22).jpg\n",
            "(512, 512)\n",
            "(73, 173, 217, 311)\n",
            "-----------------------------\n",
            "mix (22).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (23).jpg\n",
            "mix (23).jpg\n",
            "(512, 512)\n",
            "(67, 137, 151, 385)\n",
            "-----------------------------\n",
            "mix (23).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (24).jpg\n",
            "mix (24).jpg\n",
            "(512, 512)\n",
            "(102, 255, 196, 437)\n",
            "-----------------------------\n",
            "mix (24).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (25).jpg\n",
            "mix (25).jpg\n",
            "(512, 512)\n",
            "(45, 107, 200, 281)\n",
            "-----------------------------\n",
            "mix (25).jpg\n",
            "-----------------------------\n",
            "C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\mix (26).jpg\n",
            "mix (26).jpg\n",
            "(512, 512)\n",
            "(57, 210, 125, 330)\n",
            "-----------------------------\n",
            "mix (26).jpg\n"
          ]
        }
      ],
      "source": [
        "# Create a dataset instance and wrap it in a DataLoader\n",
        "root_folder = 'C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products/'  # path to your xml file\n",
        "out_folder = 'C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/CNN_finetuning/'  # path to your xml file\n",
        "\n",
        "train_dataset = RetailProductDataset(root_folder=root_folder,partition=\"train\",bbox=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "print(len(train_dataloader))\n",
        "\n",
        "\n",
        "test_dataset = RetailProductDataset(root_folder=root_folder,partition=\"test\",bbox=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "print(len(test_dataloader))\n",
        "\n",
        "# Display first item in dataloader\n",
        "for idx, data in enumerate(test_dataloader):\n",
        "    first_item = data\n",
        "    image_tensor = data[\"image\"]\n",
        "    image_tensor = test_dataset.denormalize(image_tensor)\n",
        "    image_tensor = image_tensor[0]\n",
        "    to_pil = transforms.ToPILImage()\n",
        "    image_pil = to_pil(image_tensor)\n",
        "    # print(data[\"image_path\"])\n",
        "    path_base = os.path.basename(data[\"image_path\"][0])\n",
        "    print(path_base)\n",
        "    image_pil.save(f'{out_folder}/test/{path_base}')\n",
        "    # if idx > 10:\n",
        "    #     break\n",
        "    \n",
        "dataloaders_dict = {\"train\":train_dataloader, \"test\":test_dataloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"C:/Users/Chiqu/Documents/GitHub/Deep-Learning-and-Computer-Vision-for-Business/02-Pytorch and CV/datasets/retail_products//images/test\\pepsodent (1).jpg\"\n",
        "\n",
        "# pepsodent (1).jpg\n",
        "# (512, 512)\n",
        "# (95, 252, 444, 359)\n",
        "image = Image.open(image_path)\n",
        "image.show()  # This will display the cropped image\n",
        "\n",
        "cropped_image  = image.crop((60, 200, 444, 300))\n",
        "cropped_image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gutfmMfkCWzj"
      },
      "source": [
        "### Explanation of the `train_model` function\n",
        "\n",
        "This function is designed to train a deep learning model using PyTorch. It handles the training and validation phases for multiple epochs and tracks the best-performing model based on validation accuracy. Below is an explanation of each part of the function:\n",
        "\n",
        "1. **Function Arguments:**\n",
        "   - `model`: The model to be trained.\n",
        "   - `dataloaders`: A dictionary containing 'train' and 'val' dataloaders, which provide batches of data for training and validation.\n",
        "   - `criterion`: The loss function to be optimized.\n",
        "   - `optimizer`: The optimization algorithm used to update model weights (e.g., SGD, Adam).\n",
        "   - `num_epochs`: The number of epochs (iterations over the entire dataset) to train the model (default: 25).\n",
        "   - `is_inception`: A flag indicating whether the model is Inception, as it has special handling due to auxiliary outputs during training.\n",
        "\n",
        "2. **Initial Setup:**\n",
        "   - `since`: Records the starting time to measure how long training takes.\n",
        "   - `val_acc_history`: A list to store the validation accuracy at the end of each epoch.\n",
        "   - `best_model_wts`: Deep copy of the model's initial weights, used to save the best-performing model.\n",
        "   - `best_acc`: Keeps track of the highest validation accuracy achieved.\n",
        "\n",
        "3. **Training Loop:**\n",
        "   - The outer loop runs over a number of epochs.\n",
        "   - For each epoch, it prints the current epoch and separates the training and validation phases.\n",
        "   - For each phase ('train' or 'val'), it either sets the model to training mode (`model.train()`) or evaluation mode (`model.eval()`).\n",
        "\n",
        "4. **Batch Loop:**\n",
        "   - The inner loop iterates over batches of input data (`inputs`) and their corresponding labels (`labels`).\n",
        "   - The inputs and labels are moved to the device (e.g., GPU or CPU).\n",
        "   - The gradients of the model are reset using `optimizer.zero_grad()`.\n",
        "\n",
        "5. **Forward Pass:**\n",
        "   - In the forward pass, the model computes predictions on the inputs.\n",
        "   - For Inception models, the loss is calculated using both the primary output and the auxiliary output during training.\n",
        "   - The loss is computed using the provided loss function (`criterion`).\n",
        "\n",
        "6. **Backward Pass & Optimization:**\n",
        "   - For the 'train' phase, backpropagation is performed (`loss.backward()`) to compute gradients, and the optimizer updates the model's parameters (`optimizer.step()`).\n",
        "\n",
        "7. **Tracking Performance:**\n",
        "   - After processing each batch, the loss and number of correct predictions are accumulated to calculate the loss and accuracy for the entire epoch.\n",
        "   - The epoch's loss and accuracy are printed for both the training and validation phases.\n",
        "\n",
        "8. **Model Checkpointing:**\n",
        "   - If the validation accuracy for the current epoch exceeds the best accuracy observed so far, the model's weights are saved as the best model.\n",
        "\n",
        "9. **Completion:**\n",
        "   - After training completes, the total training time is printed.\n",
        "   - The function returns the model with the best validation accuracy and a history of validation accuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "05lSsXiBCWzj"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for item in dataloaders[phase]:\n",
        "                # print(item)\n",
        "                inputs = item[\"image\"]\n",
        "                labels = item[\"cls\"]\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best test Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUGpVDPfCWzk"
      },
      "source": [
        "### Initializing and Reshaping the ResNet50 Network\n",
        "\n",
        "ResNet, introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), is a popular architecture for deep learning tasks. The network consists of several variants, including ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152, all available in `torchvision.models`. In this example, we focus on **ResNet50**.\n",
        "\n",
        "Since the ResNet models are pretrained on the ImageNet dataset (which has 1000 classes), the final fully connected layer (`fc`) has 1000 output features. When working with a new dataset, we need to reshape this layer to match the number of classes in the new task. \n",
        "\n",
        "For ResNet50, the final fully connected layer looks like this:\n",
        "\n",
        "```python\n",
        "(fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "```\n",
        "\n",
        "To adapt ResNet50 for our new task, we need to reinitialize `model.fc` to be a `Linear` layer with 2048 input features and the desired number of output classes, `num_classes`. Here is the code to do that:\n",
        "\n",
        "```python\n",
        "model.fc = nn.Linear(2048, num_classes)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3sPucb0fCWzk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Chiqu\\Documents\\GitHub\\Deep-Learning-and-Computer-Vision-for-Business\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Chiqu\\Documents\\GitHub\\Deep-Learning-and-Computer-Vision-for-Business\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def initialize_model(num_classes, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    input_size = 224\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(train_dataset.__datasetclasses__(), use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18hDoROYCWzl"
      },
      "source": [
        "Create the Optimizer\n",
        "--------------------\n",
        "\n",
        "Now that the model structure is correct, the final step for finetuning\n",
        "and feature extracting is to create an optimizer that only updates the\n",
        "desired parameters. Recall that after loading the pretrained model, but\n",
        "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
        "parameterâ€™s ``.requires_grad`` attributes to False. Then the\n",
        "reinitialized layerâ€™s parameters have ``.requires_grad=True`` by\n",
        "default. So now we know that *all parameters that have\n",
        ".requires_grad=True should be optimized.* Next, we make a list of such\n",
        "parameters and input this list to the SGD algorithm constructor.\n",
        "\n",
        "To verify this, check out the printed parameters to learn. When\n",
        "finetuning, this list should be long and include all of the model\n",
        "parameters. However, when feature extracting this list should be short\n",
        "and only include the weights and biases of the reshaped layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qA3Q6ZRNCWzl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "\n",
        "for name,param in model_ft.named_parameters():\n",
        "    if \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "    if param.requires_grad == True:\n",
        "        print(\"\\t\",name)\n",
        "\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGo0tOyqCWzl"
      },
      "source": [
        "Run Training and Validation Step\n",
        "--------------------------------\n",
        "\n",
        "Finally, the last step is to setup the loss for the model, then run the\n",
        "training and validation function for the set number of epochs. Notice,\n",
        "depending on the number of epochs this step may take a while on a CPU.\n",
        "Also, the default learning rate is not optimal for all of the models, so\n",
        "to achieve maximum accuracy it would be necessary to tune for each model\n",
        "separately.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XdDhfRtCCWzl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "train Loss: 2.0547 Acc: 0.1463\n",
            "test Loss: 3.5464 Acc: 0.2442\n",
            "\n",
            "Epoch 1/199\n",
            "----------\n",
            "train Loss: 2.0358 Acc: 0.1565\n",
            "test Loss: 3.3896 Acc: 0.2791\n",
            "\n",
            "Epoch 2/199\n",
            "----------\n",
            "train Loss: 2.0173 Acc: 0.1633\n",
            "test Loss: 3.3235 Acc: 0.3605\n",
            "\n",
            "Epoch 3/199\n",
            "----------\n",
            "train Loss: 2.0042 Acc: 0.1599\n",
            "test Loss: 3.3610 Acc: 0.3488\n",
            "\n",
            "Epoch 4/199\n",
            "----------\n",
            "train Loss: 1.9902 Acc: 0.1735\n",
            "test Loss: 3.1791 Acc: 0.3605\n",
            "\n",
            "Epoch 5/199\n",
            "----------\n",
            "train Loss: 1.9737 Acc: 0.1803\n",
            "test Loss: 3.2267 Acc: 0.3605\n",
            "\n",
            "Epoch 6/199\n",
            "----------\n",
            "train Loss: 1.9620 Acc: 0.1803\n",
            "test Loss: 3.1210 Acc: 0.3488\n",
            "\n",
            "Epoch 7/199\n",
            "----------\n",
            "train Loss: 1.9444 Acc: 0.1871\n",
            "test Loss: 2.9424 Acc: 0.3488\n",
            "\n",
            "Epoch 8/199\n",
            "----------\n",
            "train Loss: 1.9307 Acc: 0.1871\n",
            "test Loss: 2.9781 Acc: 0.3488\n",
            "\n",
            "Epoch 9/199\n",
            "----------\n",
            "train Loss: 1.9127 Acc: 0.1871\n",
            "test Loss: 2.8452 Acc: 0.3605\n",
            "\n",
            "Epoch 10/199\n",
            "----------\n",
            "train Loss: 1.9024 Acc: 0.1871\n",
            "test Loss: 2.9598 Acc: 0.3488\n",
            "\n",
            "Epoch 11/199\n",
            "----------\n",
            "train Loss: 1.8880 Acc: 0.1905\n",
            "test Loss: 2.8181 Acc: 0.3837\n",
            "\n",
            "Epoch 12/199\n",
            "----------\n",
            "train Loss: 1.8747 Acc: 0.1905\n",
            "test Loss: 2.8653 Acc: 0.3837\n",
            "\n",
            "Epoch 13/199\n",
            "----------\n",
            "train Loss: 1.8669 Acc: 0.1973\n",
            "test Loss: 2.8467 Acc: 0.3953\n",
            "\n",
            "Epoch 14/199\n",
            "----------\n",
            "train Loss: 1.8470 Acc: 0.1973\n",
            "test Loss: 2.8103 Acc: 0.4070\n",
            "\n",
            "Epoch 15/199\n",
            "----------\n",
            "train Loss: 1.8302 Acc: 0.2075\n",
            "test Loss: 2.7367 Acc: 0.4186\n",
            "\n",
            "Epoch 16/199\n",
            "----------\n",
            "train Loss: 1.8123 Acc: 0.2177\n",
            "test Loss: 2.7064 Acc: 0.3953\n",
            "\n",
            "Epoch 17/199\n",
            "----------\n",
            "train Loss: 1.8072 Acc: 0.2177\n",
            "test Loss: 2.6834 Acc: 0.4070\n",
            "\n",
            "Epoch 18/199\n",
            "----------\n",
            "train Loss: 1.7895 Acc: 0.2177\n",
            "test Loss: 2.7293 Acc: 0.4070\n",
            "\n",
            "Epoch 19/199\n",
            "----------\n",
            "train Loss: 1.7814 Acc: 0.2279\n",
            "test Loss: 2.5883 Acc: 0.4535\n",
            "\n",
            "Epoch 20/199\n",
            "----------\n",
            "train Loss: 1.7630 Acc: 0.2517\n",
            "test Loss: 2.7685 Acc: 0.4186\n",
            "\n",
            "Epoch 21/199\n",
            "----------\n",
            "train Loss: 1.7540 Acc: 0.2415\n",
            "test Loss: 2.6324 Acc: 0.4302\n",
            "\n",
            "Epoch 22/199\n",
            "----------\n",
            "train Loss: 1.7372 Acc: 0.2517\n",
            "test Loss: 2.6381 Acc: 0.4302\n",
            "\n",
            "Epoch 23/199\n",
            "----------\n",
            "train Loss: 1.7281 Acc: 0.2415\n",
            "test Loss: 2.6648 Acc: 0.4651\n",
            "\n",
            "Epoch 24/199\n",
            "----------\n",
            "train Loss: 1.7164 Acc: 0.2483\n",
            "test Loss: 2.6220 Acc: 0.4419\n",
            "\n",
            "Epoch 25/199\n",
            "----------\n",
            "train Loss: 1.7006 Acc: 0.2823\n",
            "test Loss: 2.9154 Acc: 0.4419\n",
            "\n",
            "Epoch 26/199\n",
            "----------\n",
            "train Loss: 1.6939 Acc: 0.2823\n",
            "test Loss: 2.6180 Acc: 0.4419\n",
            "\n",
            "Epoch 27/199\n",
            "----------\n",
            "train Loss: 1.6725 Acc: 0.3163\n",
            "test Loss: 2.5654 Acc: 0.4535\n",
            "\n",
            "Epoch 28/199\n",
            "----------\n",
            "train Loss: 1.6506 Acc: 0.3095\n",
            "test Loss: 2.8280 Acc: 0.4419\n",
            "\n",
            "Epoch 29/199\n",
            "----------\n",
            "train Loss: 1.6579 Acc: 0.3027\n",
            "test Loss: 2.7613 Acc: 0.4419\n",
            "\n",
            "Epoch 30/199\n",
            "----------\n",
            "train Loss: 1.6424 Acc: 0.3265\n",
            "test Loss: 2.7778 Acc: 0.4651\n",
            "\n",
            "Epoch 31/199\n",
            "----------\n",
            "train Loss: 1.6276 Acc: 0.3197\n",
            "test Loss: 2.6975 Acc: 0.4651\n",
            "\n",
            "Epoch 32/199\n",
            "----------\n",
            "train Loss: 1.6211 Acc: 0.3503\n",
            "test Loss: 2.6010 Acc: 0.4767\n",
            "\n",
            "Epoch 33/199\n",
            "----------\n",
            "train Loss: 1.6191 Acc: 0.3197\n",
            "test Loss: 2.7575 Acc: 0.4419\n",
            "\n",
            "Epoch 34/199\n",
            "----------\n",
            "train Loss: 1.5999 Acc: 0.3673\n",
            "test Loss: 2.9288 Acc: 0.4651\n",
            "\n",
            "Epoch 35/199\n",
            "----------\n",
            "train Loss: 1.5812 Acc: 0.3571\n",
            "test Loss: 2.8701 Acc: 0.4419\n",
            "\n",
            "Epoch 36/199\n",
            "----------\n",
            "train Loss: 1.5643 Acc: 0.3912\n",
            "test Loss: 2.8266 Acc: 0.4535\n",
            "\n",
            "Epoch 37/199\n",
            "----------\n",
            "train Loss: 1.5651 Acc: 0.3741\n",
            "test Loss: 2.6298 Acc: 0.4884\n",
            "\n",
            "Epoch 38/199\n",
            "----------\n",
            "train Loss: 1.5550 Acc: 0.3844\n",
            "test Loss: 2.8965 Acc: 0.4651\n",
            "\n",
            "Epoch 39/199\n",
            "----------\n",
            "train Loss: 1.5344 Acc: 0.4252\n",
            "test Loss: 2.6674 Acc: 0.4767\n",
            "\n",
            "Epoch 40/199\n",
            "----------\n",
            "train Loss: 1.5215 Acc: 0.4422\n",
            "test Loss: 2.4489 Acc: 0.5349\n",
            "\n",
            "Epoch 41/199\n",
            "----------\n",
            "train Loss: 1.5284 Acc: 0.4048\n",
            "test Loss: 2.9275 Acc: 0.4535\n",
            "\n",
            "Epoch 42/199\n",
            "----------\n",
            "train Loss: 1.5097 Acc: 0.4116\n",
            "test Loss: 3.1289 Acc: 0.4535\n",
            "\n",
            "Epoch 43/199\n",
            "----------\n",
            "train Loss: 1.4880 Acc: 0.4456\n",
            "test Loss: 3.0312 Acc: 0.4767\n",
            "\n",
            "Epoch 44/199\n",
            "----------\n",
            "train Loss: 1.4802 Acc: 0.4320\n",
            "test Loss: 2.8023 Acc: 0.4767\n",
            "\n",
            "Epoch 45/199\n",
            "----------\n",
            "train Loss: 1.4817 Acc: 0.4388\n",
            "test Loss: 2.7043 Acc: 0.5116\n",
            "\n",
            "Epoch 46/199\n",
            "----------\n",
            "train Loss: 1.4585 Acc: 0.4796\n",
            "test Loss: 2.6880 Acc: 0.5116\n",
            "\n",
            "Epoch 47/199\n",
            "----------\n",
            "train Loss: 1.4524 Acc: 0.4660\n",
            "test Loss: 2.5633 Acc: 0.5465\n",
            "\n",
            "Epoch 48/199\n",
            "----------\n",
            "train Loss: 1.4287 Acc: 0.4966\n",
            "test Loss: 3.3017 Acc: 0.4419\n",
            "\n",
            "Epoch 49/199\n",
            "----------\n",
            "train Loss: 1.4383 Acc: 0.4558\n",
            "test Loss: 2.9231 Acc: 0.5116\n",
            "\n",
            "Epoch 50/199\n",
            "----------\n",
            "train Loss: 1.3976 Acc: 0.5204\n",
            "test Loss: 2.7228 Acc: 0.5349\n",
            "\n",
            "Epoch 51/199\n",
            "----------\n",
            "train Loss: 1.4168 Acc: 0.4830\n",
            "test Loss: 2.9890 Acc: 0.5116\n",
            "\n",
            "Epoch 52/199\n",
            "----------\n",
            "train Loss: 1.3916 Acc: 0.5340\n",
            "test Loss: 2.7631 Acc: 0.5233\n",
            "\n",
            "Epoch 53/199\n",
            "----------\n",
            "train Loss: 1.3917 Acc: 0.5272\n",
            "test Loss: 2.6844 Acc: 0.5698\n",
            "\n",
            "Epoch 54/199\n",
            "----------\n",
            "train Loss: 1.4044 Acc: 0.4898\n",
            "test Loss: 2.6049 Acc: 0.5698\n",
            "\n",
            "Epoch 55/199\n",
            "----------\n",
            "train Loss: 1.3687 Acc: 0.5510\n",
            "test Loss: 2.7200 Acc: 0.5698\n",
            "\n",
            "Epoch 56/199\n",
            "----------\n",
            "train Loss: 1.3720 Acc: 0.5102\n",
            "test Loss: 2.6242 Acc: 0.5581\n",
            "\n",
            "Epoch 57/199\n",
            "----------\n",
            "train Loss: 1.3557 Acc: 0.5408\n",
            "test Loss: 2.7721 Acc: 0.5698\n",
            "\n",
            "Epoch 58/199\n",
            "----------\n",
            "train Loss: 1.3362 Acc: 0.5408\n",
            "test Loss: 2.9138 Acc: 0.5349\n",
            "\n",
            "Epoch 59/199\n",
            "----------\n",
            "train Loss: 1.3391 Acc: 0.5272\n",
            "test Loss: 3.0270 Acc: 0.5581\n",
            "\n",
            "Epoch 60/199\n",
            "----------\n",
            "train Loss: 1.3225 Acc: 0.5442\n",
            "test Loss: 2.9133 Acc: 0.5465\n",
            "\n",
            "Epoch 61/199\n",
            "----------\n",
            "train Loss: 1.3023 Acc: 0.5850\n",
            "test Loss: 3.1550 Acc: 0.5233\n",
            "\n",
            "Epoch 62/199\n",
            "----------\n",
            "train Loss: 1.2972 Acc: 0.5918\n",
            "test Loss: 2.9310 Acc: 0.5465\n",
            "\n",
            "Epoch 63/199\n",
            "----------\n",
            "train Loss: 1.2985 Acc: 0.5748\n",
            "test Loss: 2.6694 Acc: 0.5698\n",
            "\n",
            "Epoch 64/199\n",
            "----------\n",
            "train Loss: 1.2996 Acc: 0.5816\n",
            "test Loss: 3.1997 Acc: 0.5116\n",
            "\n",
            "Epoch 65/199\n",
            "----------\n",
            "train Loss: 1.3067 Acc: 0.5850\n",
            "test Loss: 3.3855 Acc: 0.5116\n",
            "\n",
            "Epoch 66/199\n",
            "----------\n",
            "train Loss: 1.2748 Acc: 0.5918\n",
            "test Loss: 3.1076 Acc: 0.5581\n",
            "\n",
            "Epoch 67/199\n",
            "----------\n",
            "train Loss: 1.2915 Acc: 0.5884\n",
            "test Loss: 3.3937 Acc: 0.5233\n",
            "\n",
            "Epoch 68/199\n",
            "----------\n",
            "train Loss: 1.2721 Acc: 0.5782\n",
            "test Loss: 2.8333 Acc: 0.5581\n",
            "\n",
            "Epoch 69/199\n",
            "----------\n",
            "train Loss: 1.2410 Acc: 0.5986\n",
            "test Loss: 2.1824 Acc: 0.6163\n",
            "\n",
            "Epoch 70/199\n",
            "----------\n",
            "train Loss: 1.2410 Acc: 0.6054\n",
            "test Loss: 3.0564 Acc: 0.5698\n",
            "\n",
            "Epoch 71/199\n",
            "----------\n",
            "train Loss: 1.2383 Acc: 0.6156\n",
            "test Loss: 3.0327 Acc: 0.5465\n",
            "\n",
            "Epoch 72/199\n",
            "----------\n",
            "train Loss: 1.2195 Acc: 0.6293\n",
            "test Loss: 3.2837 Acc: 0.5465\n",
            "\n",
            "Epoch 73/199\n",
            "----------\n",
            "train Loss: 1.2220 Acc: 0.6327\n",
            "test Loss: 2.9385 Acc: 0.5930\n",
            "\n",
            "Epoch 74/199\n",
            "----------\n",
            "train Loss: 1.1954 Acc: 0.6327\n",
            "test Loss: 2.8385 Acc: 0.5581\n",
            "\n",
            "Epoch 75/199\n",
            "----------\n",
            "train Loss: 1.1891 Acc: 0.6565\n",
            "test Loss: 2.6309 Acc: 0.5930\n",
            "\n",
            "Epoch 76/199\n",
            "----------\n",
            "train Loss: 1.2105 Acc: 0.6259\n",
            "test Loss: 3.0222 Acc: 0.5465\n",
            "\n",
            "Epoch 77/199\n",
            "----------\n",
            "train Loss: 1.1800 Acc: 0.6531\n",
            "test Loss: 2.5546 Acc: 0.6047\n",
            "\n",
            "Epoch 78/199\n",
            "----------\n",
            "train Loss: 1.2109 Acc: 0.6122\n",
            "test Loss: 2.8036 Acc: 0.5814\n",
            "\n",
            "Epoch 79/199\n",
            "----------\n",
            "train Loss: 1.1867 Acc: 0.6667\n",
            "test Loss: 2.7416 Acc: 0.5814\n",
            "\n",
            "Epoch 80/199\n",
            "----------\n",
            "train Loss: 1.1666 Acc: 0.6224\n",
            "test Loss: 3.1606 Acc: 0.5814\n",
            "\n",
            "Epoch 81/199\n",
            "----------\n",
            "train Loss: 1.1556 Acc: 0.6497\n",
            "test Loss: 2.5901 Acc: 0.5930\n",
            "\n",
            "Epoch 82/199\n",
            "----------\n",
            "train Loss: 1.1347 Acc: 0.6939\n",
            "test Loss: 3.3896 Acc: 0.5814\n",
            "\n",
            "Epoch 83/199\n",
            "----------\n",
            "train Loss: 1.1301 Acc: 0.6633\n",
            "test Loss: 2.8317 Acc: 0.5698\n",
            "\n",
            "Epoch 84/199\n",
            "----------\n",
            "train Loss: 1.1440 Acc: 0.6463\n",
            "test Loss: 2.9746 Acc: 0.6047\n",
            "\n",
            "Epoch 85/199\n",
            "----------\n",
            "train Loss: 1.1258 Acc: 0.6667\n",
            "test Loss: 3.2614 Acc: 0.5698\n",
            "\n",
            "Epoch 86/199\n",
            "----------\n",
            "train Loss: 1.1480 Acc: 0.6667\n",
            "test Loss: 2.7528 Acc: 0.5930\n",
            "\n",
            "Epoch 87/199\n",
            "----------\n",
            "train Loss: 1.1171 Acc: 0.6735\n",
            "test Loss: 2.9210 Acc: 0.6047\n",
            "\n",
            "Epoch 88/199\n",
            "----------\n",
            "train Loss: 1.1346 Acc: 0.6565\n",
            "test Loss: 2.7445 Acc: 0.6279\n",
            "\n",
            "Epoch 89/199\n",
            "----------\n",
            "train Loss: 1.0920 Acc: 0.7041\n",
            "test Loss: 3.1536 Acc: 0.5814\n",
            "\n",
            "Epoch 90/199\n",
            "----------\n",
            "train Loss: 1.1142 Acc: 0.6735\n",
            "test Loss: 2.7826 Acc: 0.6163\n",
            "\n",
            "Epoch 91/199\n",
            "----------\n",
            "train Loss: 1.0692 Acc: 0.6905\n",
            "test Loss: 3.5154 Acc: 0.5349\n",
            "\n",
            "Epoch 92/199\n",
            "----------\n",
            "train Loss: 1.1088 Acc: 0.6531\n",
            "test Loss: 2.8096 Acc: 0.5581\n",
            "\n",
            "Epoch 93/199\n",
            "----------\n",
            "train Loss: 1.0711 Acc: 0.7007\n",
            "test Loss: 3.0402 Acc: 0.5814\n",
            "\n",
            "Epoch 94/199\n",
            "----------\n",
            "train Loss: 1.0683 Acc: 0.6667\n",
            "test Loss: 3.0754 Acc: 0.5814\n",
            "\n",
            "Epoch 95/199\n",
            "----------\n",
            "train Loss: 1.0471 Acc: 0.7347\n",
            "test Loss: 2.8173 Acc: 0.5930\n",
            "\n",
            "Epoch 96/199\n",
            "----------\n",
            "train Loss: 1.0612 Acc: 0.7007\n",
            "test Loss: 2.8690 Acc: 0.6163\n",
            "\n",
            "Epoch 97/199\n",
            "----------\n",
            "train Loss: 1.0793 Acc: 0.6667\n",
            "test Loss: 3.5661 Acc: 0.5698\n",
            "\n",
            "Epoch 98/199\n",
            "----------\n",
            "train Loss: 1.0598 Acc: 0.6905\n",
            "test Loss: 3.0128 Acc: 0.5814\n",
            "\n",
            "Epoch 99/199\n",
            "----------\n",
            "train Loss: 1.0535 Acc: 0.7177\n",
            "test Loss: 3.2495 Acc: 0.5698\n",
            "\n",
            "Epoch 100/199\n",
            "----------\n",
            "train Loss: 1.0383 Acc: 0.7347\n",
            "test Loss: 3.3257 Acc: 0.5698\n",
            "\n",
            "Epoch 101/199\n",
            "----------\n",
            "train Loss: 1.0475 Acc: 0.7007\n",
            "test Loss: 3.3602 Acc: 0.5930\n",
            "\n",
            "Epoch 102/199\n",
            "----------\n",
            "train Loss: 0.9890 Acc: 0.7551\n",
            "test Loss: 2.7421 Acc: 0.6279\n",
            "\n",
            "Epoch 103/199\n",
            "----------\n",
            "train Loss: 1.0206 Acc: 0.7041\n",
            "test Loss: 2.8956 Acc: 0.5698\n",
            "\n",
            "Epoch 104/199\n",
            "----------\n",
            "train Loss: 1.0007 Acc: 0.7007\n",
            "test Loss: 3.2937 Acc: 0.5698\n",
            "\n",
            "Epoch 105/199\n",
            "----------\n",
            "train Loss: 0.9943 Acc: 0.7415\n",
            "test Loss: 3.0996 Acc: 0.5930\n",
            "\n",
            "Epoch 106/199\n",
            "----------\n",
            "train Loss: 1.0189 Acc: 0.7177\n",
            "test Loss: 2.6575 Acc: 0.6395\n",
            "\n",
            "Epoch 107/199\n",
            "----------\n",
            "train Loss: 1.0312 Acc: 0.6837\n",
            "test Loss: 3.3771 Acc: 0.5698\n",
            "\n",
            "Epoch 108/199\n",
            "----------\n",
            "train Loss: 1.0102 Acc: 0.7245\n",
            "test Loss: 3.2556 Acc: 0.6047\n",
            "\n",
            "Epoch 109/199\n",
            "----------\n",
            "train Loss: 0.9963 Acc: 0.7313\n",
            "test Loss: 3.5376 Acc: 0.5814\n",
            "\n",
            "Epoch 110/199\n",
            "----------\n",
            "train Loss: 0.9686 Acc: 0.7415\n",
            "test Loss: 2.9824 Acc: 0.6047\n",
            "\n",
            "Epoch 111/199\n",
            "----------\n",
            "train Loss: 0.9705 Acc: 0.7279\n",
            "test Loss: 3.1034 Acc: 0.5930\n",
            "\n",
            "Epoch 112/199\n",
            "----------\n",
            "train Loss: 1.0189 Acc: 0.6973\n",
            "test Loss: 3.3666 Acc: 0.5814\n",
            "\n",
            "Epoch 113/199\n",
            "----------\n",
            "train Loss: 0.9758 Acc: 0.7041\n",
            "test Loss: 2.9516 Acc: 0.6047\n",
            "\n",
            "Epoch 114/199\n",
            "----------\n",
            "train Loss: 0.9692 Acc: 0.7347\n",
            "test Loss: 3.3000 Acc: 0.5698\n",
            "\n",
            "Epoch 115/199\n",
            "----------\n",
            "train Loss: 0.9384 Acc: 0.7857\n",
            "test Loss: 3.0686 Acc: 0.5698\n",
            "\n",
            "Epoch 116/199\n",
            "----------\n",
            "train Loss: 0.9391 Acc: 0.7551\n",
            "test Loss: 2.9798 Acc: 0.6047\n",
            "\n",
            "Epoch 117/199\n",
            "----------\n",
            "train Loss: 0.9490 Acc: 0.7619\n",
            "test Loss: 2.7824 Acc: 0.5930\n",
            "\n",
            "Epoch 118/199\n",
            "----------\n",
            "train Loss: 0.9572 Acc: 0.7449\n",
            "test Loss: 4.1702 Acc: 0.5698\n",
            "\n",
            "Epoch 119/199\n",
            "----------\n",
            "train Loss: 0.9357 Acc: 0.7483\n",
            "test Loss: 3.4357 Acc: 0.6047\n",
            "\n",
            "Epoch 120/199\n",
            "----------\n",
            "train Loss: 0.9379 Acc: 0.7551\n",
            "test Loss: 3.1963 Acc: 0.5930\n",
            "\n",
            "Epoch 121/199\n",
            "----------\n",
            "train Loss: 0.9504 Acc: 0.7415\n",
            "test Loss: 2.7895 Acc: 0.6279\n",
            "\n",
            "Epoch 122/199\n",
            "----------\n",
            "train Loss: 0.9005 Acc: 0.7721\n",
            "test Loss: 3.2210 Acc: 0.5698\n",
            "\n",
            "Epoch 123/199\n",
            "----------\n",
            "train Loss: 0.9151 Acc: 0.7721\n",
            "test Loss: 2.9103 Acc: 0.6047\n",
            "\n",
            "Epoch 124/199\n",
            "----------\n",
            "train Loss: 0.8751 Acc: 0.7789\n",
            "test Loss: 2.8095 Acc: 0.6163\n",
            "\n",
            "Epoch 125/199\n",
            "----------\n",
            "train Loss: 0.9322 Acc: 0.7891\n",
            "test Loss: 3.3586 Acc: 0.6047\n",
            "\n",
            "Epoch 126/199\n",
            "----------\n",
            "train Loss: 0.8809 Acc: 0.7857\n",
            "test Loss: 2.7168 Acc: 0.6395\n",
            "\n",
            "Epoch 127/199\n",
            "----------\n",
            "train Loss: 0.9101 Acc: 0.7925\n",
            "test Loss: 3.6484 Acc: 0.6163\n",
            "\n",
            "Epoch 128/199\n",
            "----------\n",
            "train Loss: 0.8924 Acc: 0.7857\n",
            "test Loss: 3.3084 Acc: 0.6047\n",
            "\n",
            "Epoch 129/199\n",
            "----------\n",
            "train Loss: 0.8786 Acc: 0.7993\n",
            "test Loss: 3.0370 Acc: 0.6395\n",
            "\n",
            "Epoch 130/199\n",
            "----------\n",
            "train Loss: 0.8707 Acc: 0.7721\n",
            "test Loss: 3.2311 Acc: 0.6047\n",
            "\n",
            "Epoch 131/199\n",
            "----------\n",
            "train Loss: 0.9018 Acc: 0.7721\n",
            "test Loss: 3.5277 Acc: 0.5930\n",
            "\n",
            "Epoch 132/199\n",
            "----------\n",
            "train Loss: 0.8561 Acc: 0.7789\n",
            "test Loss: 3.2070 Acc: 0.5930\n",
            "\n",
            "Epoch 133/199\n",
            "----------\n",
            "train Loss: 0.8811 Acc: 0.8061\n",
            "test Loss: 2.5057 Acc: 0.6512\n",
            "\n",
            "Epoch 134/199\n",
            "----------\n",
            "train Loss: 0.8417 Acc: 0.8197\n",
            "test Loss: 3.5983 Acc: 0.5930\n",
            "\n",
            "Epoch 135/199\n",
            "----------\n",
            "train Loss: 0.8805 Acc: 0.7891\n",
            "test Loss: 3.4858 Acc: 0.5930\n",
            "\n",
            "Epoch 136/199\n",
            "----------\n",
            "train Loss: 0.8696 Acc: 0.7857\n",
            "test Loss: 3.6820 Acc: 0.6163\n",
            "\n",
            "Epoch 137/199\n",
            "----------\n",
            "train Loss: 0.8587 Acc: 0.7687\n",
            "test Loss: 3.0313 Acc: 0.6047\n",
            "\n",
            "Epoch 138/199\n",
            "----------\n",
            "train Loss: 0.8569 Acc: 0.7925\n",
            "test Loss: 3.1909 Acc: 0.6279\n",
            "\n",
            "Epoch 139/199\n",
            "----------\n",
            "train Loss: 0.8320 Acc: 0.7993\n",
            "test Loss: 3.0886 Acc: 0.6163\n",
            "\n",
            "Epoch 140/199\n",
            "----------\n",
            "train Loss: 0.8222 Acc: 0.8061\n",
            "test Loss: 3.1969 Acc: 0.6395\n",
            "\n",
            "Epoch 141/199\n",
            "----------\n",
            "train Loss: 0.8376 Acc: 0.7959\n",
            "test Loss: 3.5527 Acc: 0.6047\n",
            "\n",
            "Epoch 142/199\n",
            "----------\n",
            "train Loss: 0.8612 Acc: 0.7925\n",
            "test Loss: 3.6924 Acc: 0.5814\n",
            "\n",
            "Epoch 143/199\n",
            "----------\n",
            "train Loss: 0.8433 Acc: 0.7687\n",
            "test Loss: 3.3137 Acc: 0.6163\n",
            "\n",
            "Epoch 144/199\n",
            "----------\n",
            "train Loss: 0.8384 Acc: 0.7925\n",
            "test Loss: 2.5298 Acc: 0.6512\n",
            "\n",
            "Epoch 145/199\n",
            "----------\n",
            "train Loss: 0.7965 Acc: 0.8435\n",
            "test Loss: 3.2124 Acc: 0.6163\n",
            "\n",
            "Epoch 146/199\n",
            "----------\n",
            "train Loss: 0.8242 Acc: 0.7891\n",
            "test Loss: 3.6392 Acc: 0.6047\n",
            "\n",
            "Epoch 147/199\n",
            "----------\n",
            "train Loss: 0.8175 Acc: 0.7653\n",
            "test Loss: 3.7253 Acc: 0.5930\n",
            "\n",
            "Epoch 148/199\n",
            "----------\n",
            "train Loss: 0.8173 Acc: 0.7755\n",
            "test Loss: 2.7983 Acc: 0.6512\n",
            "\n",
            "Epoch 149/199\n",
            "----------\n",
            "train Loss: 0.7980 Acc: 0.7959\n",
            "test Loss: 3.4705 Acc: 0.6279\n",
            "\n",
            "Epoch 150/199\n",
            "----------\n",
            "train Loss: 0.7905 Acc: 0.8197\n",
            "test Loss: 3.6061 Acc: 0.6047\n",
            "\n",
            "Epoch 151/199\n",
            "----------\n",
            "train Loss: 0.8270 Acc: 0.7925\n",
            "test Loss: 3.0743 Acc: 0.6395\n",
            "\n",
            "Epoch 152/199\n",
            "----------\n",
            "train Loss: 0.7811 Acc: 0.8333\n",
            "test Loss: 3.2742 Acc: 0.6279\n",
            "\n",
            "Epoch 153/199\n",
            "----------\n",
            "train Loss: 0.7998 Acc: 0.7891\n",
            "test Loss: 3.6020 Acc: 0.6047\n",
            "\n",
            "Epoch 154/199\n",
            "----------\n",
            "train Loss: 0.7774 Acc: 0.8061\n",
            "test Loss: 3.1100 Acc: 0.6395\n",
            "\n",
            "Epoch 155/199\n",
            "----------\n",
            "train Loss: 0.7964 Acc: 0.7959\n",
            "test Loss: 3.8890 Acc: 0.5814\n",
            "\n",
            "Epoch 156/199\n",
            "----------\n",
            "train Loss: 0.7677 Acc: 0.8333\n",
            "test Loss: 3.3049 Acc: 0.6395\n",
            "\n",
            "Epoch 157/199\n",
            "----------\n",
            "train Loss: 0.7625 Acc: 0.8401\n",
            "test Loss: 3.2924 Acc: 0.6512\n",
            "\n",
            "Epoch 158/199\n",
            "----------\n",
            "train Loss: 0.7584 Acc: 0.8299\n",
            "test Loss: 3.8767 Acc: 0.6047\n",
            "\n",
            "Epoch 159/199\n",
            "----------\n",
            "train Loss: 0.7656 Acc: 0.7993\n",
            "test Loss: 4.1513 Acc: 0.5698\n",
            "\n",
            "Epoch 160/199\n",
            "----------\n",
            "train Loss: 0.7663 Acc: 0.8129\n",
            "test Loss: 3.7904 Acc: 0.5814\n",
            "\n",
            "Epoch 161/199\n",
            "----------\n",
            "train Loss: 0.7654 Acc: 0.8095\n",
            "test Loss: 3.7159 Acc: 0.6279\n",
            "\n",
            "Epoch 162/199\n",
            "----------\n",
            "train Loss: 0.7896 Acc: 0.7993\n",
            "test Loss: 4.0566 Acc: 0.5930\n",
            "\n",
            "Epoch 163/199\n",
            "----------\n",
            "train Loss: 0.7613 Acc: 0.8129\n",
            "test Loss: 3.4582 Acc: 0.6512\n",
            "\n",
            "Epoch 164/199\n",
            "----------\n",
            "train Loss: 0.7907 Acc: 0.7925\n",
            "test Loss: 3.6128 Acc: 0.6279\n",
            "\n",
            "Epoch 165/199\n",
            "----------\n",
            "train Loss: 0.7490 Acc: 0.7959\n",
            "test Loss: 4.6271 Acc: 0.5814\n",
            "\n",
            "Epoch 166/199\n",
            "----------\n",
            "train Loss: 0.7310 Acc: 0.8095\n",
            "test Loss: 3.6092 Acc: 0.6395\n",
            "\n",
            "Epoch 167/199\n",
            "----------\n",
            "train Loss: 0.7109 Acc: 0.8537\n",
            "test Loss: 3.5558 Acc: 0.6163\n",
            "\n",
            "Epoch 168/199\n",
            "----------\n",
            "train Loss: 0.7041 Acc: 0.8605\n",
            "test Loss: 3.4863 Acc: 0.6163\n",
            "\n",
            "Epoch 169/199\n",
            "----------\n",
            "train Loss: 0.7261 Acc: 0.8231\n",
            "test Loss: 3.0656 Acc: 0.6279\n",
            "\n",
            "Epoch 170/199\n",
            "----------\n",
            "train Loss: 0.7522 Acc: 0.8027\n",
            "test Loss: 3.3250 Acc: 0.6395\n",
            "\n",
            "Epoch 171/199\n",
            "----------\n",
            "train Loss: 0.7601 Acc: 0.8197\n",
            "test Loss: 3.6722 Acc: 0.5930\n",
            "\n",
            "Epoch 172/199\n",
            "----------\n",
            "train Loss: 0.7456 Acc: 0.8129\n",
            "test Loss: 3.4361 Acc: 0.6512\n",
            "\n",
            "Epoch 173/199\n",
            "----------\n",
            "train Loss: 0.7347 Acc: 0.8367\n",
            "test Loss: 3.5553 Acc: 0.6279\n",
            "\n",
            "Epoch 174/199\n",
            "----------\n",
            "train Loss: 0.7306 Acc: 0.8197\n",
            "test Loss: 3.9693 Acc: 0.6163\n",
            "\n",
            "Epoch 175/199\n",
            "----------\n",
            "train Loss: 0.7006 Acc: 0.8537\n",
            "test Loss: 3.4142 Acc: 0.6395\n",
            "\n",
            "Epoch 176/199\n",
            "----------\n",
            "train Loss: 0.7042 Acc: 0.8231\n",
            "test Loss: 3.4174 Acc: 0.6395\n",
            "\n",
            "Epoch 177/199\n",
            "----------\n",
            "train Loss: 0.7252 Acc: 0.7959\n",
            "test Loss: 2.8379 Acc: 0.6628\n",
            "\n",
            "Epoch 178/199\n",
            "----------\n",
            "train Loss: 0.7168 Acc: 0.8163\n",
            "test Loss: 3.7417 Acc: 0.6163\n",
            "\n",
            "Epoch 179/199\n",
            "----------\n",
            "train Loss: 0.7153 Acc: 0.8231\n",
            "test Loss: 4.0350 Acc: 0.6047\n",
            "\n",
            "Epoch 180/199\n",
            "----------\n",
            "train Loss: 0.7037 Acc: 0.8061\n",
            "test Loss: 2.7851 Acc: 0.6395\n",
            "\n",
            "Epoch 181/199\n",
            "----------\n",
            "train Loss: 0.6987 Acc: 0.8129\n",
            "test Loss: 3.0639 Acc: 0.6395\n",
            "\n",
            "Epoch 182/199\n",
            "----------\n",
            "train Loss: 0.7050 Acc: 0.8367\n",
            "test Loss: 3.5397 Acc: 0.5930\n",
            "\n",
            "Epoch 183/199\n",
            "----------\n",
            "train Loss: 0.6983 Acc: 0.8129\n",
            "test Loss: 3.1218 Acc: 0.6628\n",
            "\n",
            "Epoch 184/199\n",
            "----------\n",
            "train Loss: 0.7142 Acc: 0.8231\n",
            "test Loss: 3.0505 Acc: 0.6512\n",
            "\n",
            "Epoch 185/199\n",
            "----------\n",
            "train Loss: 0.7038 Acc: 0.8367\n",
            "test Loss: 3.2914 Acc: 0.6744\n",
            "\n",
            "Epoch 186/199\n",
            "----------\n",
            "train Loss: 0.6859 Acc: 0.8333\n",
            "test Loss: 3.6720 Acc: 0.6279\n",
            "\n",
            "Epoch 187/199\n",
            "----------\n",
            "train Loss: 0.6470 Acc: 0.8503\n",
            "test Loss: 2.9525 Acc: 0.6628\n",
            "\n",
            "Epoch 188/199\n",
            "----------\n",
            "train Loss: 0.6585 Acc: 0.8367\n",
            "test Loss: 3.5149 Acc: 0.6395\n",
            "\n",
            "Epoch 189/199\n",
            "----------\n",
            "train Loss: 0.7051 Acc: 0.8299\n",
            "test Loss: 3.0281 Acc: 0.6744\n",
            "\n",
            "Epoch 190/199\n",
            "----------\n",
            "train Loss: 0.7050 Acc: 0.8163\n",
            "test Loss: 3.3222 Acc: 0.6512\n",
            "\n",
            "Epoch 191/199\n",
            "----------\n",
            "train Loss: 0.6443 Acc: 0.8605\n",
            "test Loss: 3.6185 Acc: 0.6279\n",
            "\n",
            "Epoch 192/199\n",
            "----------\n",
            "train Loss: 0.6606 Acc: 0.8435\n",
            "test Loss: 3.6855 Acc: 0.6279\n",
            "\n",
            "Epoch 193/199\n",
            "----------\n",
            "train Loss: 0.6368 Acc: 0.8571\n",
            "test Loss: 4.0600 Acc: 0.6279\n",
            "\n",
            "Epoch 194/199\n",
            "----------\n",
            "train Loss: 0.6485 Acc: 0.8707\n",
            "test Loss: 4.1134 Acc: 0.5930\n",
            "\n",
            "Epoch 195/199\n",
            "----------\n",
            "train Loss: 0.6258 Acc: 0.8605\n",
            "test Loss: 4.0190 Acc: 0.6163\n",
            "\n",
            "Epoch 196/199\n",
            "----------\n",
            "train Loss: 0.6732 Acc: 0.8401\n",
            "test Loss: 4.0863 Acc: 0.5930\n",
            "\n",
            "Epoch 197/199\n",
            "----------\n",
            "train Loss: 0.6461 Acc: 0.8537\n",
            "test Loss: 3.3479 Acc: 0.6744\n",
            "\n",
            "Epoch 198/199\n",
            "----------\n",
            "train Loss: 0.6784 Acc: 0.8265\n",
            "test Loss: 3.3087 Acc: 0.6279\n",
            "\n",
            "Epoch 199/199\n",
            "----------\n",
            "train Loss: 0.6490 Acc: 0.8707\n",
            "test Loss: 3.5391 Acc: 0.6395\n",
            "\n",
            "Training complete in 28m 40s\n",
            "Best test Acc: 0.674419\n"
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, device=device, num_epochs=200)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
